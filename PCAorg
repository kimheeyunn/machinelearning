
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate
from sklearn.cluster import KMeans

# 과일 데이터 다운로드 하기
!wget https://bit.ly/fruits_300_data -O fruits_300.npy

fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1,100 * 100)

# 주성분 : 전체 데이터의 분산을 가장 잘 설명하는 성분
# 분산이 커져야 데이터 사이의 차이점이 명확해짐

pca = PCA(n_components = 50) # 몇 개의 특성으로 데이터의 차원을 줄일지
pca.fit(fruits_2d)


print(pca.components_.shape)

def draw_fruits(arr, ratio=1):
  n = len(arr)
  rows = int(np.ceil(n/10))
  cols = n if rows < 2 else 10
  fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze = False)

  for i in range(rows):
    for j in range(cols):
      if i*10 + j < n:
        axs[i,j].imshow(arr[i*10+j], cmap='gray_r')
      axs[i, j].axis('off')
  plt.show()

draw_fruits(pca.components_.reshape(-1,100,100))

print(fruits_2d.shape)
# 새로운 주성분으로 데이터 변환
# 차원이 축소된 것을 확인할 수 있음

fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)

# 주성분 분석으로 인해 축소된 차원을 다시 복구
fruits_inverse = pca.inverse_transform(fruits_pca)
print(fruits_inverse.shape)

fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)

for i in [0, 100, 200]:
  draw_fruits(fruits_reconstruct[i:i+100])
  print('\n')

# 주성분 벡터가 이루는 축에 투영한 결과의 분산의 비율
# 50개의 주성분이 전체 분산의 92%를 설명하고 있음을 알 수 있음

print(pca.explained_variance_ratio_)
print(np.sum(pca.explained_variance_ratio_))

plt.plot(pca.explained_variance_ratio_)

lr = LogisticRegression()
target = np.array([0]*100 + [1]*100 + [2]*100)
# 모델 반복 학습(교차 검증) : 주성분 분석 X

# cross_validate(평가모델, 훈련 데이터, 시험 데이터)
scores = cross_validate(lr, fruits_2d, target)

print(np.mean(scores['test_score'])) # 평가 점수
print(np.mean(scores['fit_time'])) # 학습 시간
